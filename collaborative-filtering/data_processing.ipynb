{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Review Data for Collaborative Filtering\n",
    "\n",
    "**Author**: Stella Zarei <br>\n",
    "**Created**: 2025/03/02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'McAuley-Lab/Amazon-Reviews-2023'\n",
    "SELECTED_CATEGORY = 'Video_Games'\n",
    "\n",
    "dataset = datasets.load_dataset(DATASET, 'raw_review_'+SELECTED_CATEGORY, trust_remote_code=True)\n",
    "raw_df = dataset['full'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                              title  \\\n",
      "0     4.0                     It’s pretty sexual. Not my fav   \n",
      "1     5.0                                   Good. A bit slow   \n",
      "2     5.0  ... an order for my kids & they have really en...   \n",
      "3     5.0                        Great alt to pro controller   \n",
      "4     5.0                                      solid product   \n",
      "\n",
      "                                                text  product_id  \\\n",
      "0  I’m playing on ps5 and it’s interesting.  It’s...  B07DK1H3H5   \n",
      "1  Nostalgic fun.  A bit slow.  I hope they don’t...  B07SRWRH5D   \n",
      "2  This was an order for my kids & they have real...  B07MFMFW34   \n",
      "3  These work great, They use batteries which is ...  B0BCHWZX95   \n",
      "4  I would recommend to anyone looking to add jus...  B00HUWA45W   \n",
      "\n",
      "                        user_id      timestamp  helpful_vote  \n",
      "0  AGCI7FAH4GL5FI65HYLKWTMFZ2CQ  1608186804795             0  \n",
      "1  AGCI7FAH4GL5FI65HYLKWTMFZ2CQ  1587051114941             1  \n",
      "2  AGXVBIUFLFGMVLATYXHJYL4A5Q7Q  1490877431000             0  \n",
      "3  AFTC6ZR5IKNRDG5JCPVNVMU3XV2Q  1577637634017             0  \n",
      "4  AFTC6ZR5IKNRDG5JCPVNVMU3XV2Q  1427591932000             0  \n"
     ]
    }
   ],
   "source": [
    "# Remove unwanted columns\n",
    "raw_df.drop(columns=['images', 'asin', 'verified_purchase'], inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "raw_df.rename(columns={'parent_asin': 'product_id'}, inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "raw_df.drop_duplicates(['user_id', 'product_id'], inplace=True)\n",
    "\n",
    "print(raw_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforce Minimum Interaction Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, min_interactions=5):\n",
    "    while True:\n",
    "        user_counts = df['user_id'].value_counts()\n",
    "        df = df[df['user_id'].isin(user_counts[user_counts >= min_interactions].index)]\n",
    "\n",
    "        item_counts = df['product_id'].value_counts()\n",
    "        df = df[df['product_id'].isin(item_counts[item_counts >= min_interactions].index)]\n",
    "\n",
    "        # Check if filtering is complete\n",
    "        if all(user_counts >= min_interactions) and all(item_counts >= min_interactions):\n",
    "            break\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "filtered_df = filter_data(raw_df, min_interactions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical User and Product ID\n",
    "\n",
    "This step is completed after the \"inactive\" users and products have been filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                              title  \\\n",
      "0     4.0               Nice consistent coloring. Rough edge   \n",
      "1     5.0                         Nice quality for the money   \n",
      "2     4.0  Comfortable, Nice fit, no other accessories th...   \n",
      "3     5.0      Really like the controller look with these on   \n",
      "4     4.0   Looks good, not sure how functional it is though   \n",
      "\n",
      "                                                text  product_id  user_id  \\\n",
      "0  I really like how consistent the bright was is...           0        0   \n",
      "1  I ordered these in black and white and they bo...           1        0   \n",
      "2  This is pretty comfortable and seems to do the...           2        0   \n",
      "3  The straps are very functional. I do wish they...           3        0   \n",
      "4  See pics, I have a Vive DAS and rubber insulat...           4        0   \n",
      "\n",
      "       timestamp  helpful_vote  \n",
      "0  1630594913298             0  \n",
      "1  1620231368468             0  \n",
      "2  1617641445475             0  \n",
      "3  1613702112995             0  \n",
      "4  1613701986538             1  \n"
     ]
    }
   ],
   "source": [
    "# Map to numerical IDs\n",
    "user_ids = filtered_df['user_id'].unique()\n",
    "encode_user_id = {id: i for i, id in enumerate(user_ids)}\n",
    "decode_user_id = {i: id for i, id in enumerate(user_ids)}\n",
    "filtered_df['user_id'] = filtered_df['user_id'].map(encode_user_id)\n",
    "\n",
    "item_ids = filtered_df['product_id'].unique()\n",
    "encode_item_id = {id: i for i, id in enumerate(item_ids)}\n",
    "decode_item_id = {i: id for i, id in enumerate(item_ids)}\n",
    "filtered_df['product_id'] = filtered_df['product_id'].map(encode_item_id)\n",
    "\n",
    "# Convert the ratings to float\n",
    "filtered_df['rating'] = filtered_df['rating'].astype(float)\n",
    "\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 94762\n",
      "Number of items: 25612\n",
      "Number of ratings: 814586\n",
      "Minimum rating: 1.0\n",
      "Maximum rating: 5.0\n",
      "Avg reviews per user: 8.60\n",
      "Avg reviews per item: 31.80\n"
     ]
    }
   ],
   "source": [
    "# Statistical summary of the dataset\n",
    "num_users = user_ids.shape[0]\n",
    "num_items = item_ids.shape[0]\n",
    "num_reviews = filtered_df.shape[0]\n",
    "min_rating = filtered_df['rating'].min()\n",
    "max_rating = filtered_df['rating'].max()\n",
    "\n",
    "print(f'Number of users: {num_users}')\n",
    "print(f'Number of items: {num_items}')\n",
    "print(f'Number of ratings: {num_reviews}')\n",
    "print(f'Minimum rating: {min_rating}')\n",
    "print(f'Maximum rating: {max_rating}')\n",
    "\n",
    "print(f'Avg reviews per user: {num_reviews/num_users:.2f}')\n",
    "print(f'Avg reviews per item: {num_reviews/num_items:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Ratings are normalized using zero-centred mean for each user. This removes the use bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the user bias - the average rating given by each user\n",
    "user_bias = filtered_df.groupby('user_id')['rating'].mean()\n",
    "\n",
    "# Normalize the ratings by subtracting the user bias\n",
    "filtered_df['rating_norm'] = filtered_df['rating'] - filtered_df['user_id'].map(user_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Dataset\n",
    "\n",
    "For user-centric collaborative filtering only a subset of the features in the raw data is required. The full data set can be condensed to only include `user_id` `product_id` `timestamp` and `rating_norm`.\n",
    "\n",
    "The filtering performed in previous steps insures that any `user_id` in the dataset is associated to a minimum of ratings of $N$. <br>\n",
    "To partition the training and test sets we will take $T$ examples into the test set, $V$ examples into the validation set and the remaining $N-T-V$ examples into the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: X=(625062, 3), y=(625062,)\n",
      "Validation set: X=(94762, 3), y=(94762,)\n",
      "Test set: X=(94762, 3), y=(94762,)\n"
     ]
    }
   ],
   "source": [
    "# Partition size\n",
    "num_test = 1\n",
    "num_val = 1\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_set = []\n",
    "val_set = []\n",
    "test_set = []\n",
    "\n",
    "# Extract the reviews for each user and assign to partitions\n",
    "for user, user_reviews in filtered_df.groupby('user_id'):\n",
    "    # order the reviews by timestamp\n",
    "    user_reviews = user_reviews.sort_values('timestamp')\n",
    "    # most recent reviews are used for testing\n",
    "    test_set.append(user_reviews.iloc[-num_test:])\n",
    "    # second most recent reviews are used for validation\n",
    "    val_set.append(user_reviews.iloc[-num_test-num_val:-num_test])\n",
    "    # the rest are used for training\n",
    "    train_set.append(user_reviews.iloc[:-num_test-num_val])\n",
    "\n",
    "# Concatenate the partitions and reset the index\n",
    "train_set = pd.concat(train_set).reset_index(drop=True)\n",
    "val_set = pd.concat(val_set).reset_index(drop=True)\n",
    "test_set = pd.concat(test_set).reset_index(drop=True)\n",
    "\n",
    "# Separate the features and label\n",
    "X_train = train_set[['user_id', 'product_id', 'timestamp']].values\n",
    "y_train = train_set['rating_norm'].values\n",
    "\n",
    "X_val = val_set[['user_id', 'product_id', 'timestamp']].values\n",
    "y_val = val_set['rating_norm'].values\n",
    "\n",
    "X_test = test_set[['user_id', 'product_id', 'timestamp']].values\n",
    "y_test = test_set['rating_norm'].values\n",
    "\n",
    "# Print the shape of the datasets\n",
    "print(f'Training set: X={X_train.shape}, y={y_train.shape}')\n",
    "print(f'Validation set: X={X_val.shape}, y={y_val.shape}')\n",
    "print(f'Test set: X={X_test.shape}, y={y_test.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
