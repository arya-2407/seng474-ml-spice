{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aryap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aryap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'McAuley-Lab/Amazon-Reviews-2023'\n",
    "SELECTED_CATEGORY = 'Video_Games'\n",
    "\n",
    "dataset = datasets.load_dataset(DATASET, 'raw_review_'+SELECTED_CATEGORY, trust_remote_code=True)\n",
    "raw_df = dataset['full'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.rename(columns={'parent_asin': 'product_id'}, inplace=True)\n",
    "\n",
    "# Remove duplicates (same as original)\n",
    "raw_df.drop_duplicates(['user_id', 'product_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enforce Minimum Interaction Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df, min_interactions=5):\n",
    "    while True:\n",
    "        user_counts = df['user_id'].value_counts()\n",
    "        df = df[df['user_id'].isin(user_counts[user_counts >= min_interactions].index)]\n",
    "\n",
    "        item_counts = df['product_id'].value_counts()\n",
    "        df = df[df['product_id'].isin(item_counts[item_counts >= min_interactions].index)]\n",
    "\n",
    "        # Check if filtering is complete\n",
    "        if (user_counts[df['user_id'].unique()].min() >= min_interactions and \n",
    "            item_counts[df['product_id'].unique()].min() >= min_interactions):\n",
    "            break\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "filtered_df = filter_data(raw_df, min_interactions=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical User and Product ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = filtered_df['user_id'].unique()\n",
    "encode_user_id = {id: i for i, id in enumerate(user_ids)}\n",
    "decode_user_id = {i: id for i, id in enumerate(user_ids)}\n",
    "filtered_df['user_id_numeric'] = filtered_df['user_id'].map(encode_user_id)\n",
    "\n",
    "item_ids = filtered_df['product_id'].unique()\n",
    "encode_item_id = {id: i for i, id in enumerate(item_ids)}\n",
    "decode_item_id = {i: id for i, id in enumerate(item_ids)}\n",
    "filtered_df['product_id_numeric'] = filtered_df['product_id'].map(encode_item_id)\n",
    "\n",
    "# Convert the ratings to float\n",
    "filtered_df['rating'] = filtered_df['rating'].astype(float)\n",
    "\n",
    "# Create a simplified dataset for TF-IDF clustering\n",
    "tfidf_dataset = filtered_df[['user_id_numeric', 'product_id_numeric', \n",
    "                            'user_id', 'product_id',\n",
    "                            'title', 'text', 'rating', \n",
    "                            'timestamp', 'helpful_vote']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, numbers, and extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace special chars with space\n",
    "    text = re.sub(r'\\d+', ' ', text)      # Replace numbers with space\n",
    "    text = re.sub(r'\\s+', ' ', text)      # Replace multiple spaces with single space\n",
    "    text = text.strip()                   # Remove leading/trailing whitespace\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryap\\AppData\\Local\\Temp\\ipykernel_14032\\1225458435.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tfidf_dataset['title_clean'] = tfidf_dataset['title'].fillna('').apply(clean_text)\n",
      "C:\\Users\\aryap\\AppData\\Local\\Temp\\ipykernel_14032\\1225458435.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tfidf_dataset['text_clean'] = tfidf_dataset['text'].fillna('').apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "tfidf_dataset['title_clean'] = tfidf_dataset['title'].fillna('').apply(clean_text)\n",
    "tfidf_dataset['text_clean'] = tfidf_dataset['text'].fillna('').apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dataset['combined_text'] = (tfidf_dataset['title_clean'] + ' ' + \n",
    "                                 tfidf_dataset['text_clean'])\n",
    "tfidf_dataset['processed_text'] = tfidf_dataset['combined_text'].apply(tokenize_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 113741\n",
      "Number of items: 28113\n",
      "Number of reviews: 893040\n",
      "Avg reviews per user: 7.85\n",
      "Avg reviews per item: 31.77\n"
     ]
    }
   ],
   "source": [
    "num_users = user_ids.shape[0]\n",
    "num_items = item_ids.shape[0]\n",
    "num_reviews = filtered_df.shape[0]\n",
    "\n",
    "print(f'Number of users: {num_users}')\n",
    "print(f'Number of items: {num_items}')\n",
    "print(f'Number of reviews: {num_reviews}')\n",
    "print(f'Avg reviews per user: {num_reviews/num_users:.2f}')\n",
    "print(f'Avg reviews per item: {num_reviews/num_items:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dataset.to_parquet('amazon_reviews_for_tfidf.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
